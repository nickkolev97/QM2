{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxouC6cfjAYL"
      },
      "source": [
        "# Natural Language Processing\n",
        "\n",
        "## *Workshop 4*  [![Open In Colab](https://github.com/oballinger/QM2/blob/main/colab-badge.png?raw=1)](https://colab.research.google.com/github/oballinger/QM2/blob/main/notebooks/W04.%20Natural%20Language%20Processing.ipynb)\n",
        "\n",
        "Today we'll be using the *Natural Language Tool Kit* package **nltk**, which will allow us to split (clean) text into words, parts of speech, and sentences, and plot word occurrence and frequency.\n",
        "\n",
        "**Aims**\n",
        "\n",
        "- to work with nltk and some standard corpus texts\n",
        "- to tokenise by word and sentence\n",
        "- to plot word occurrence and frequency\n",
        "- to filter by parts of speech"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Background\n",
        "\n",
        "Exxon Mobil is the 4th largest oil company in the world. In 1978, an Exxon scientist named James Black wrote an [internal briefing](https://insideclimatenews.org/documents/james-black-1977-presentation/) called \"The Greenhouse Effect\" in which he warned: “Present thinking holds that man has a time window of five to ten years before the need for hard decisions regarding changes in energy strategies might become critical.” \n",
        "\n",
        "Rather than acting on this information, Exxon spent the next [forty years aggressively funding climate denial](https://news.harvard.edu/gazette/story/2021/09/oil-companies-discourage-climate-action-study-says/). Recently, [a U.S. court ruled](https://www.theguardian.com/environment/2022/may/24/exxon-trial-climate-crimes-fossil-fuels-global-heating) that ExxonMobil must face trial over accusations that it lied about the climate crisis and covered up the fossil fuel industry’s role in worsening environmental devastation.\n",
        "\n",
        "### Earnings Calls\n",
        "Every three months, Exxon conducts an [\"earnings call\"](https://www.investopedia.com/terms/e/earnings-call.asp); a conference call between the management of a public company, analysts, investors, and the media to discuss the company’s financial results during a given reporting period, such as a quarter or a fiscal year. \n",
        "\n",
        "You can [register](https://globalmeet.webcasts.com/starthere.jsp?ei=1488251&tp_key=440e363aaf) to attend their next one if you want! No worries if you miss it, they provide [transcripts](https://corporate.exxonmobil.com/Investors/Investor-relations/Investor-materials-archive#Quarterlyearningsmaterials) on their website.\n",
        "\n",
        "These transcripts provide an intimate window into the company's \n",
        ". We can see how much pressure investors are putting on the company to tackle climate change, and how the company responds. \n",
        "\n",
        "We'll be working with transcripts spanning nealry 20 years and over 10 million words; that's like reading the Harry Potter series 10 times. "
      ],
      "metadata": {
        "id": "3sLO5n6tnemE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://haha.business/business.jpg)\n"
      ],
      "metadata": {
        "id": "zIhVuwMlbvQC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LD1Au6yHbr2q"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZwtwz53jAYM"
      },
      "source": [
        "## Downloading the Data\n",
        "Let's grab the data we will need this week from our course website and save it into our data folder. If you've not already created a data folder then do so using the following command. \n",
        "\n",
        "Don't worry if it generates an error, that means you've already got a data folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8OXlIf2jAYN",
        "outputId": "743421ff-2461-4c88-f48d-7488484b8150",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.9.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textacy in /usr/local/lib/python3.7/dist-packages (0.11.0)\n",
            "Requirement already satisfied: spacy>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (3.4.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.7.3)\n",
            "Requirement already satisfied: jellyfish>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.7/dist-packages (from textacy) (4.64.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (2.6.3)\n",
            "Requirement already satisfied: scikit-learn>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.0.2)\n",
            "Requirement already satisfied: cytoolz>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from textacy) (0.12.0)\n",
            "Requirement already satisfied: pyphen>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (0.13.0)\n",
            "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.21.6)\n",
            "Requirement already satisfied: cachetools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (4.2.4)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (2.23.0)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from cytoolz>=0.10.1->textacy) (0.12.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (2022.9.24)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.0->textacy) (3.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (21.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (1.0.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (1.9.2)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (4.1.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (3.0.10)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.4.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (8.1.4)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.6.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (57.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (3.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (1.0.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.10.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.0.0->textacy) (3.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=3.0.0->textacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=3.0.0->textacy) (5.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.0.0->textacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.0.0->textacy) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0.0->textacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.0.0->textacy) (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scattertext in /usr/local/lib/python3.7/dist-packages (0.1.7)\n",
            "Requirement already satisfied: flashtext in /usr/local/lib/python3.7/dist-packages (from scattertext) (2.7)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from scattertext) (0.12.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.0.2)\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.7/dist-packages (from scattertext) (4.0.3)\n",
            "Requirement already satisfied: gensim>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from scattertext) (4.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.3.5)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=4.0.0->scattertext) (5.2.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->scattertext) (2022.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->scattertext) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->scattertext) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->scattertext) (1.2.0)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.7/dist-packages (from statsmodels->scattertext) (0.5.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tika in /usr/local/lib/python3.7/dist-packages (1.24)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tika) (57.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tika) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (2022.9.24)\n"
          ]
        }
      ],
      "source": [
        "#Make a ./data directory\n",
        "!pip install spacy\n",
        "!pip install textacy\n",
        "!pip install nltk\n",
        "!pip install scattertext\n",
        "!pip install tika"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeYFCcv4jAYT"
      },
      "outputs": [],
      "source": [
        "#Make a ./data/wk4 directory\n",
        "!mkdir data/wk4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXB0zCQEjAYY"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import json\n",
        "import pylab\n",
        "from IPython.core.display import display, HTML\n",
        "import nltk\n",
        "import tika \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "nltk.download('stopwords')\n",
        "pylab.rcParams['figure.figsize'] = (10., 8.)\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exxon host earnings calls on their website in PDF form. Usually, working with PDFs is a real pain as they are not machine-readable. Using a python package called [tika](https://www.geeksforgeeks.org/parsing-pdfs-in-python-with-tika/), we can \"parse\" a pdf, turning it into machine-readable text:"
      ],
      "metadata": {
        "id": "eU1g5loJ2_3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the URL where your PDF lives. You could also upload your own pdf.\n",
        "url='https://corporate.exxonmobil.com/-/media/Global/Files/investor-relations/quarterly-earnings/earnings-transcripts/2022-earnings-transcripts/1Q22-XOM-Earnings-Call-Transcript-4-29-22.pdf'\n",
        "\n",
        "# parse the pdf by feeding tika the URL and store the text in an object called \"raw\" \n",
        "raw = parser.from_file(url)\n"
      ],
      "metadata": {
        "id": "ibXsDF1uW_dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we have an object called \"raw\" that contains some useful information. Notice the squiggly brackets; this is a dictionary. It contains several fields, including some useful metadata such as the author"
      ],
      "metadata": {
        "id": "g4Wevbuz4Q8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "date=raw['metadata']['date']\n",
        "title=raw['metadata']['dc:title']\n",
        "raw_text=raw['content']\n",
        "\n",
        "print('Date: ', date)\n",
        "print('Title: ', title)\n",
        "print('Word Count: ', len(raw_text))\n",
        "print('Text:')\n",
        "raw_text"
      ],
      "metadata": {
        "id": "1na8WczX4RQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "look at that! we're beginning to give some structure to our text data. But suppose I wanted to analyze multiple earnings calls; I need to organize this data so that it can accomodate new entries. As always, we want to **tabularize** our data. Let's create a dataframe with three columns (Date, Title, and Text) in which each row is one earnings call:"
      ],
      "metadata": {
        "id": "UPC9xxqr8dji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dataframe using the above data \n",
        "call=pd.DataFrame({'Date':[date],'Title':[title],'Text':[raw_text]})\n",
        "\n",
        "# remember, datetime information almost always reaches us as text. \n",
        "# we need to explicitly convert it to the datetime data type. \n",
        "call['Date']=pd.to_datetime(call['Date'], infer_datetime_format=True)\n",
        "\n",
        "# Let's see what we've got.\n",
        "call"
      ],
      "metadata": {
        "id": "QgbT-rjW7-Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, if we were so inclined, we could use a loop to repeat this process for a large number of earnings calls, yielding a neatly organized dataframe containing the date, title, and text of earnings calls over time. I've done this so you don't have to, and stored it as a file called \"Exxon.json\". It spans 2002-2019, and contains over 10 million words' worth of earnings calls. Let's take a peek:"
      ],
      "metadata": {
        "id": "Br3d1ibYBQYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_json('data/wk4/Exxon.json')\n",
        "df"
      ],
      "metadata": {
        "id": "ZAcADSi_BRUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great-- we've got a structured dataset of earnings calls. But even though the data has *structure*, the data in the \"Text\" column still needs some cleaning and processing. "
      ],
      "metadata": {
        "id": "BsEn8Sq_BqVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dirty Words\n",
        "\n",
        "Text often comes 'unclean' either containing tags such as HTML (or XML), or has other issues.\n",
        "We've already done a bit of tidying, but it's been relatively straightforward. Be cautious when committing to a text analysis project - you may spend a great deal of time tidying up your text. \n",
        "\n",
        "For example, you may have noticed \"\\n\\n\\n\\n\\n\\n\\n\\n...\" in the text of the first earnings call we downloaded. This is a character (just like \"a\" or \"$\") except it indicates that we want to create a new line. It's part of the formatting of the pdf. That's not really useful information to us. Let start by selecting an earnings call; i've chosen the 38th in this dataframe:"
      ],
      "metadata": {
        "id": "ELfPVOKy6jh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "call=df.iloc[38]\n",
        "\n",
        "print('Date: ', call['Date'])\n",
        "print('Title: ', call['Title'])\n",
        "print('Word Count: ', len(call['Text']))\n",
        "print('Text:')\n",
        "call['Text']"
      ],
      "metadata": {
        "id": "lliAXrWoCXsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This call took place on May 25th, 2016. The transcript is over 125,000 words, nearly as long as the third Lord of the Rings book. It would be a pain to read all of it, so we'll use python to extract insights. \n",
        "\n",
        "The kind of analysis we will be doing reqires *tokenizing* a text, and *tagging* individual words. Tokenizing means splitting the text into individul sentences or individual words, while tagging means classifying each word according to a POS (Parts Of Speech) classification. We can word tokenize our data using the `word_tokenize` function from the `nltk` library, which returns an object which is a list of tokens; if we print the first 19 tokens, we get the first sentence: "
      ],
      "metadata": {
        "id": "Yg_ohnHgC_3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  tokens=nltk.word_tokenize(call['Text'])  \n",
        "  tokens[:19]"
      ],
      "metadata": {
        "id": "SnOvPKe36lEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lovely! the first sentence is an introduction by then-CEO [Rex Tillerson](https://en.wikipedia.org/wiki/Rex_Tillerson).\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/53/Rex_Tillerson_official_portrait.jpg/800px-Rex_Tillerson_official_portrait.jpg.jpg\" alt=\"drawing\" width=\"200\"/>\n",
        "\n",
        "He was CEO of Exxon from 2006 until he retired on January 1st 2017. One month later, he was sworn in as U.S. Secretary of State under Donald Trump. Let's see what Rex thinks about climate change!\n",
        "\n",
        "Having tokenized our text, there are a number of useful functions can use to explore it quantitatively. Let's start by counting the number of times \"climate\" is mentioned:\n",
        "\n"
      ],
      "metadata": {
        "id": "iu_vj_B5-RYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens.count('climate')"
      ],
      "metadata": {
        "id": "h6y7ZqsGAhe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.Text(tokens).collocations()\n"
      ],
      "metadata": {
        "id": "TYoZWfH_Wzz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.app import concordance\n",
        "import re\n",
        "len(re.findall('carbon capture', call['Text']))\n",
        "re.findall(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b', call['Text'])\n",
        "#nltk.Text(tokens).concordance('climate change')"
      ],
      "metadata": {
        "id": "ozQ_thdfYerG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import bigrams\n",
        "\n",
        "finder = BigramCollocationFinder.from_words(tokens)\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "print(finder.nbest(bigram_measures.pmi, 2))"
      ],
      "metadata": {
        "id": "do60msoVkxhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(raw_text):\n",
        "  tokens=nltk.word_tokenize(raw_text)\n",
        "  text=nltk.Text(tokens)\n",
        "  return text\n",
        "\n",
        "df['Text']=df['Text'].apply(lambda x: tokenize(x))"
      ],
      "metadata": {
        "id": "5qi2IUdweY2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['climate']=df['Text'].apply(lambda x: x.count('climate'))\n",
        "df.plot('climate')"
      ],
      "metadata": {
        "id": "Io-GB2ZZi8ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "call=df.iloc[38]\n",
        "\n",
        "text=call['Text']\n",
        "#tokens=nltk.word_tokenize(raw_text)\n",
        "#text=nltk.Text(tokens)\n",
        "\n",
        "word_fd = nltk.FreqDist(filtered_sentence)\n",
        "\n",
        "text.concordance('climate')\n",
        "print(text.count('climate change'))\n",
        "\n",
        "text.dispersion_plot(['climate'])"
      ],
      "metadata": {
        "id": "pGJ4ToaRZRWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scatter Text"
      ],
      "metadata": {
        "id": "z1roMPrccSr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ceo_df=df[30:40]\n",
        "ceo_df['CEO']=np.where(ceo_df['Date']>'01-01-2017','Woods','Tillerson')\n",
        "\n",
        "import scattertext as st\n",
        "\n",
        "corpus = st.CorpusFromPandas(ceo_df,\n",
        "                             category_col='CEO',\n",
        "                             text_col='Text',\n",
        "                             nlp=nlp).build()"
      ],
      "metadata": {
        "id": "Ej6_Xvfc-XnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=corpus.remove_terms(nlp.Defaults.stop_words, ignore_absences=True)\n"
      ],
      "metadata": {
        "id": "8jbKhYc0UhrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "html = st.produce_scattertext_explorer(\n",
        "                   corpus,\n",
        "                   category='Woods',\n",
        "                   category_name='Woods',\n",
        "                   not_category_name='Tillerson',\n",
        "                   width_in_pixels=1000)\n",
        "\n",
        "display(HTML(html))"
      ],
      "metadata": {
        "id": "7dAj7ndkQq-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aFHaxXaqYZKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Text']=df['Text'].str.split('\\n\\n')\n",
        "paragraphs=df.explode('Text')\n",
        "paragraphs"
      ],
      "metadata": {
        "id": "LOjMeLpR-py1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs['speaker']=paragraphs['Text'].str.split(':').str[0]\n",
        "paragraphs"
      ],
      "metadata": {
        "id": "c7xZlfraXCtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "matcher.add(\"cc\", [nlp(\"carbon capture\")])\n",
        "matches = matcher(doc)\n",
        "\n",
        "ix=matches[0]\n",
        "\n",
        "doc[ix[1]-10:ix[2]+10]"
      ],
      "metadata": {
        "id": "2gaNqz7mwqTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "phrase_matcher = PhraseMatcher(nlp.vocab)\n",
        "phrase_list = [nlp('climate change')]\n",
        "phrase_matcher.add(\"Text Extractor\", None, *phrase_list)\n",
        "matched_items = phrase_matcher(doc)\n",
        "\n",
        "matched_text = []\n",
        "\n",
        "for match_id, start, end in matched_items:\n",
        "    text = nlp.vocab.strings[match_id]\n",
        "    span = doc[start: end]\n",
        "    matched_text.append(span.sent)\n",
        "\n",
        "matched_text"
      ],
      "metadata": {
        "id": "NPOS6hnSuDI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(matched_text[1], jupyter=True)"
      ],
      "metadata": {
        "id": "f2nP5YEIHRJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textacy.extract\n",
        "\n",
        "kwic=textacy.extract.kwic.keyword_in_context(doc, 'global warming')\n",
        "\n",
        "for a in kwic:\n",
        "  print(a)\n"
      ],
      "metadata": {
        "id": "JpQOHPaDh-l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "displacy.render(sentences[1], jupyter=True)"
      ],
      "metadata": {
        "id": "x_33mSS7lRIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, defaultdict\n",
        "\n",
        "def find_character_occurences(doc):\n",
        "    \"\"\"\n",
        "    Return a list of actors from `doc` with corresponding occurences.\n",
        "    \n",
        "    :param doc: Spacy NLP parsed document\n",
        "    :return: list of tuples in form\n",
        "        [('elizabeth', 622), ('darcy', 312), ('jane', 286), ('bennet', 266)]\n",
        "    \"\"\"\n",
        "    \n",
        "    characters = Counter()\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == 'PERSON':\n",
        "            characters[ent.lemma_] += 1\n",
        "            \n",
        "    return characters.most_common()\n",
        "\n",
        "print(find_character_occurences(doc)[:20])"
      ],
      "metadata": {
        "id": "hoEde2YviZs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text.dispersion_plot(['Grail','rabbit','Knights','Ni','castle'])"
      ],
      "metadata": {
        "id": "AgA6Clq1q8Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.ent_)"
      ],
      "metadata": {
        "id": "GSkzIbW5XSUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAVvtEX6jAYb"
      },
      "source": [
        "## Dirty Words\n",
        "\n",
        "Text often comes 'unclean' either containing tags such as HTML (or XML), or has other issues, but fortunately we will be using 'clean' sources, at least initially. Be cautious when committing to a text analysis project - you may spend a great deal of time tidying up your text.\n",
        "\n",
        "The kind of analysis we will be doing reqires *tokenizing* a text, and *tagging* individual words. Tokenizing means splitting the text into individul sentences or individual words, while tagging means classifying each word according to a POS (Parts Of Speech) classification. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2uw3HT0jAYb"
      },
      "source": [
        "## The Castle of Aaargh\n",
        "We will first experiment with nltk and its built in corpus texts. We'll work with some Monty Python, beloved of comedy bores for half a century"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMgq_w5CjAYc"
      },
      "source": [
        "## Setup\n",
        "\n",
        "- install nltk through package manager, or the command line\n",
        "- import nltk\n",
        "- type nltk.download('book'). This will automatically download the books into our workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X14YfA2ejAYd"
      },
      "outputs": [],
      "source": [
        "#nltk: natural language processing toolkit\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ao7y2Gd_jAYf"
      },
      "outputs": [],
      "source": [
        "#Download the sample books\n",
        "nltk.download('book')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaD9wjhzjAYi"
      },
      "source": [
        "Now, we import the sample texts. You'll notice that text6 is \"Monty Python and the Holy Grail\", as promised. Presumably this was compiled pre-*Spamalot*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsoVoCHejAYj"
      },
      "outputs": [],
      "source": [
        "#Import all features from nltk.book\n",
        "from nltk.book import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvnDgXfgjAYl"
      },
      "source": [
        "Let's look at the object text6; we can look at the first few words.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uV0fZDKvjAYm"
      },
      "outputs": [],
      "source": [
        "#View the first ten words of text6\n",
        "text6[1:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxGMtiBBjAYo"
      },
      "source": [
        "Which are presumably stage directions rather than dialogue. How many words/symbols are in the text?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0D3vDc8xjAYp"
      },
      "outputs": [],
      "source": [
        "#View the length of the list\n",
        "len(text6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRw32-UFjAYr"
      },
      "source": [
        "Bear in mind that a lot of the functions we will carry out rely on this being a text object - we'll start to think about how we use free text and convert it to a **Text** object later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unXAzIInjAYr"
      },
      "source": [
        "We can now start to do some slightly more sophisticated work; for example, a dispersion plot to see where words appear. Let's give it a few keywords that those familiar with *...The Holy Grail* might recognise:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsaM5FEljAYs"
      },
      "outputs": [],
      "source": [
        "text6.dispersion_plot(['Grail','rabbit','Knights','Ni','castle'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAu_-1JEjAYt"
      },
      "source": [
        "And we can easily count *how many* times a word appears."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upaRb9iwjAYu"
      },
      "outputs": [],
      "source": [
        "text6.count('Ni')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2AACARjjAYx"
      },
      "source": [
        "Or the words which most commonly appear together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4mNYR8KjAYx"
      },
      "outputs": [],
      "source": [
        "text6.collocations()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C_kwLPljAY0"
      },
      "source": [
        "## Exercise: From Hell's Heart I Stab at Thee\n",
        "From Moby Dick, find out \n",
        "- Where the narrator Ishmael, Captain Ahab and his Nemesis are mentioned. When do each enter the story? Where do they have most emphasis?\n",
        "- Which parts of the books appear to take place at sea, and points where their ship is wrecked or sinking (spoilers)\n",
        "- two significant places in the story (HINT: use collocations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmkkXMhZjAY0"
      },
      "source": [
        "Let's now look at carrying out the full process of importing and working with text data. \n",
        "\n",
        "## Making an IMPACT\n",
        "\n",
        "As part of the REF2014 exercise, universities reported on the *Impact* their research activities had on the world. Their *Impact Case Studies* were subsequently made available by HEFCE. What sort of information do they contain? How do universities frame \"impact\"? All of this data is available via the REF website.\n",
        "\n",
        "A little context: I've included examples from the four *panels* used by HEFCE. Broadly speaking, Panel A is health, bioscience and medicine, B is physical science and engineering, C is social science, and D is humanities - the full categories are visible here: \n",
        "http://www.ref.ac.uk/panels/unitsofassessment/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_jPhxxDjAY1"
      },
      "source": [
        "Let's first look at random sample from Panel A:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqMQQwwgjAY1"
      },
      "outputs": [],
      "source": [
        "#Data path to file\n",
        "data_path = \"./data/wk7/PanelA.txt\"\n",
        "\n",
        "with open(data_path) as file:\n",
        "    data = file.read()\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKKodS7djAY4"
      },
      "source": [
        "We could tokenize this into sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEVi_SoQjAY4"
      },
      "outputs": [],
      "source": [
        "sentences = nltk.sent_tokenize(data)\n",
        "sentences[1:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Gg-XygQjAY6"
      },
      "source": [
        "Or into individual words; generally, it may be useful to retain sentences, so we can see where two words are in the same sentence, for example - but we'll be doing something simpler:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5Plm1ArjAY6"
      },
      "outputs": [],
      "source": [
        "tokens = nltk.word_tokenize(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBKQuBRVjAY9"
      },
      "outputs": [],
      "source": [
        "tokens[1:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEu4tpJAjAY_"
      },
      "source": [
        "Tokenising is a process which has many subtleties and corner-cases, and you may want to proceed in a more fine-grained way for some texts:\n",
        "\n",
        "http://nltk.org/api/nltk.tokenize.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svVHPwxWjAZA"
      },
      "source": [
        "Let's now convert this into a Text object, which will allow us to analyse other aspects of the text. For example, we can look at **collocations**, words which commonly appear together. This may help to provide context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_A0kZTtjAZA"
      },
      "outputs": [],
      "source": [
        "simple_text = nltk.Text(tokens)\n",
        "simple_text.collocations()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdFk6ccAjAZC"
      },
      "source": [
        "We can create a dispersion plot - although in this case, it tells us a limited amount..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRf7MuRcjAZC"
      },
      "outputs": [],
      "source": [
        "simple_text.dispersion_plot(['NHS', 'evidence', 'practice', 'Hospital'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQixTRFfjAZE"
      },
      "source": [
        "Even from this, we get a sense of the work this unit does, and its impacts on the world. But what are the most 20 common words used? To find this out, we produce a **Freq**uency **Dist**ribution (*FreqDist*) object. This has an implicit loop - the *for* statement is telling python to look through all the words in 'tokens' and seeing how often they occur. The .lower() command converts them all to lower case for comparison, so it will flag up upper *and* lower case occurrences of the word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWurW51GjAZE"
      },
      "outputs": [],
      "source": [
        "fd = nltk.FreqDist(word.lower() for word in tokens)\n",
        "fd.plot(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmvdZO9ojAZG"
      },
      "source": [
        "Not very helpful - this includes all kinds of junk, and tells us that \"and\" is very common. Not very interesting. Let's try a bit harder and identify Parts of Speech."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oLLYkCMjAZG"
      },
      "source": [
        "## POS\n",
        "\n",
        "Parts of speech indicate whether something is a noun, a verb, adjective, and so on. In nltk, we can use the *pos_tag* command, which will identify which word belongs to which part of speech."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3BhEWJGjAZH"
      },
      "outputs": [],
      "source": [
        "tagged = nltk.pos_tag(tokens)\n",
        "tagged[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GQoRtbijAZJ"
      },
      "source": [
        "'NNP' refers to Proper Noun, Singular; you can find the full list of Parts of Speech here: http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkOv1GmojAZJ"
      },
      "outputs": [],
      "source": [
        "permitted_tags = set([\n",
        "    'NN',\n",
        "    'NNS'\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4P2wbX2jAZL"
      },
      "source": [
        "## One FOR All\n",
        "We've so far managed to avoid this staple of programming, the FOR loop - and we're not going to delve too deeply into it in the last class of term. Of course, you probably came across FOR loops and IF statements when you worked through the prerequisites for the module, but that feels like a long time ago... \n",
        "\n",
        "We do use FOR and IF here, and it's worth understanding a bit about what it means, even if you don't intend to use it a lot yourself. In the next piece of code, we set up *fd*, a new object which will record frequency distribution information. Then we use a FOR loop\n",
        "\n",
        "`for bit in tagged:\n",
        "    ...'\n",
        "\n",
        "This goes through every element of tagged one at a time - and each element is called 'bit' for the purposes of this loop. Then, for each 'bit', we check that it has one of the permitted tags, and make sure it's at least 3 characters long - shorter words probably aren't all that relevant in this case:\n",
        "\n",
        "`if bit[1] in permitted_tags and len(bit[0])>2:'\n",
        "\n",
        "note that the *and* means both of these have to be true - if both *are* true, only then does the following statement execute:\n",
        "\n",
        "`fd[bit[0]] = fd[bit[0]] + 1'\n",
        "\n",
        "which increases the count for that word. So, this code increases the count for a word iff (if and only if) its at least 3 characters long, and it's of the correct tag (Noun, Singular or Plural)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LmuzZYLjAZM"
      },
      "source": [
        "## Double Indentity\n",
        "One final remark: we haven't dealt with **indents** much in python, but indenting the code like below, after the for statement, and *again* after the if statement, is the way that python knows it's dealing with a loop (for) and a conditional (if). It's also the way python deals with defining new functions, but that's not something you will need to do. This is just a pointer - if your code doesn't work, check the colons are there (:) and the indenting is too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4xfJbyajAZN"
      },
      "source": [
        "On with the show - as promised, this creates a word frequency graph of nouns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqnTIL3SjAZN"
      },
      "outputs": [],
      "source": [
        "fd = nltk.FreqDist()\n",
        "\n",
        "for bit in tagged:\n",
        "    if bit[1] in permitted_tags and len(bit[0])>2:\n",
        "        fd[bit[0]] = fd[bit[0]] + 1\n",
        "        \n",
        "fd.plot(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E8D48iQjAZP"
      },
      "source": [
        "We start to get a sense of the impact - 'care', 'practice', and 'services' all feature heavily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUNEIrKkjAZP"
      },
      "source": [
        "Let's now look at another randomly chosen example from Panel A:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTcFZc5ejAZQ"
      },
      "outputs": [],
      "source": [
        "data_path = \"./data/wk7/PanelA2.txt\"\n",
        "\n",
        "with open(data_path) as file:\n",
        "    data = file.read()\n",
        "\n",
        "tokens = nltk.word_tokenize(text)\n",
        "simple_text.collocations()\n",
        "simple_text = nltk.Text(tokens)\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "fd = nltk.FreqDist()\n",
        "\n",
        "for bit in tagged:\n",
        "    if bit[1] in permitted_tags and len(bit[0])>2:\n",
        "        fd[bit[0]] = fd[bit[0]] + 1\n",
        "        \n",
        "fd.plot(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeqxzqwkjAZS"
      },
      "source": [
        "A very different set of words, clearly geared towards language therapy, and working directly with patients. Perhaps if we looked at the *slightly* less common words, we'd see links between these submissions -  for example, we see *communication* appearing in both. Not a huge surprise, if we're talking about public impact, but students of the public role of the university might start to wonder about the distinctions between *communication* and *engagement*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJFweobvjAZS"
      },
      "source": [
        "## Exercise\n",
        "Repeat this for the examples from Panels B, C and D - what trends and keywords appear? What use do the collocations have? What do different parts of speech (e.g. verbs or proper nouns) tell you about the text?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecSgp5_EjAZT"
      },
      "source": [
        "If we wanted to analyse the sector as a whole, we would want to analyse Impact statements en masse - and we would hope that this would draw out links across differnt statements from different centres and universities, and even in different panels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEt_3piEjAZT"
      },
      "source": [
        "## Working with larger text datasets\n",
        "Working with larger text corpora starts to get slow. At this point, we will look at a body of text we have previously tagged up.  The file is \"The Nameless City\" by H. P. Lovecraft, a horror author from the early 20th century."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZW6nG-2cjAZU"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import requests\n",
        "from urllib.request import urlopen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p574eddjAZV"
      },
      "source": [
        "This may take a little while - so wait for the task to run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdWBrdj6jAZW"
      },
      "outputs": [],
      "source": [
        "# Loading the tokenized and tagged file. \n",
        "tagged = pickle.load(urlopen(\"https://s3.eu-west-2.amazonaws.com/qm2/wk7/lovecraft_tagged.pickle\"), encoding='latin1')\n",
        "\n",
        "#How many sentences do we have?\n",
        "len(tagged)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtufCPFojAZY"
      },
      "source": [
        "This is tokenised by sentence - and there are 18,513 of them. That would have taken a long time to tag up. If you're interested, this is how you take a set of *sentences* and tag them with Part of Speech:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLb8JynVjAZY"
      },
      "outputs": [],
      "source": [
        "exampleSentences = nltk.sent_tokenize(data)\n",
        "  \n",
        "exampleTagged = [nltk.pos_tag(nltk.word_tokenize(sent)) for sent in sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooTOo3z8jAZa"
      },
      "source": [
        "Note that the second line is running an implicit for loop through every sentence, and tagging each word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cz4IfCCPjAZa"
      },
      "outputs": [],
      "source": [
        "# first sentence.\n",
        "exampleTagged[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSuvJ_TOjAZc"
      },
      "source": [
        "An impressive start! Let's again build our word frequency chart. Note now that we have an extra layer of FOR - we need to look at each sentence in the text; at each word in each sentence; and then check each word to see whether it is of an allowed type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN0d_QXCjAZc"
      },
      "outputs": [],
      "source": [
        "fd = nltk.FreqDist()\n",
        "\n",
        "permitted_tags = set([\n",
        "    'JJS',\n",
        "    'FW',\n",
        "    'NN',\n",
        "    'NNS',\n",
        "    'NNP',\n",
        "    'NNPS',\n",
        "    'UH',\n",
        "])\n",
        "for sentence in tagged:\n",
        "    for word in sentence:\n",
        "        if word[1] in permitted_tags:\n",
        "                fd[word[0]] = fd[word[0]] + 1\n",
        "fd.plot(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlvchpMQjAZe"
      },
      "source": [
        "Now that we've produced counts for all words which conform to our list of tags, we can quickly see how frequently common words appear; because we have tokenized by sentence, we have to do this with a slightly different mechanism - run through the words of interest and see how many occurrences appear in the Frequency Distribution object, fd. Again, we're sneaking in a FOR loop to run through these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkTMTA5bjAZf"
      },
      "outputs": [],
      "source": [
        "for word in ['space', 'nameless', 'mad', 'dread', 'fear', 'cthulhu', 'necronomicon', 'caring']:\n",
        "    print(word, fd[word])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1-5LR5_jAZi"
      },
      "source": [
        "So far, we've completely avoided the use of pandas - but we can put this data into a pandas dataframe very easily, and use the built-in graphing methods to change the style of our graph. \n",
        "\n",
        "We feed in fd.keys() - the words - and fd.values(), the wordcount."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJpXxOLEjAZj"
      },
      "outputs": [],
      "source": [
        "#Convert to list so subscriptable\n",
        "list(fd.keys())[1:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5WjIv1rjAZm"
      },
      "outputs": [],
      "source": [
        "#Convert to list so subscriptable\n",
        "list(fd.values())[1:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrgMyHThjAZp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Convert to list for use in creating dataframe\n",
        "df = pd.DataFrame({'items': list(fd.keys()), 'counts': list(fd.values())})\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cce-T97jAZu"
      },
      "source": [
        "Let's now arrange them in order of appearance - the most common at the top."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcIU8K-hjAZv"
      },
      "outputs": [],
      "source": [
        "df = df.sort_values(by='counts',ascending=False)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlTkVsJcjAZx"
      },
      "source": [
        "We now have a DataFrame with certain word tags sorted by decreasing frequency. Let's plot this in a bar graph; we will use df[1:50] to select the most common 50 words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfDuFypGjAZx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uy8neOGijAZ0"
      },
      "outputs": [],
      "source": [
        "plt.style.use('ggplot')\n",
        "df[1:50].plot(kind='bar', x='items', y='counts', legend=False)\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Word Count')\n",
        "plt.title('Word counts for H.P. Lovecraft\\'s \\\"The Nameless City\\\"')\n",
        "plt.axhline(df['counts'].mean(), color='#2222ff')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN8gxS-PjAZ2"
      },
      "source": [
        "## Exercise\n",
        "What is the percentage of words that appear exactly once in the entire text? (Hint : FreqDist objects have a method called 'hapaxes')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wg_R8JHjAZ3"
      },
      "source": [
        "## Exercise\n",
        "The words of our ex Prime Minister, David Cameron:\n",
        "\n",
        "    1. Select one of Cameron's speeches.\n",
        "    2. Sentence and word tokenize it.\n",
        "    3. POS tag it.\n",
        "    4. Create noun and adjective histograms of the 20 most frequent words.\n",
        "    (notice that there are several POS for each.)\n",
        "    5. Create dispersion plots for these nouns and adjectives. \n",
        "    6. What is the percentage of words that are adjectives in the speech?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-FByBrrjAZ3"
      },
      "outputs": [],
      "source": [
        "# read a raw text from a remote location. \n",
        "speech = requests.get('https://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2006a.txt').text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9zFG5MUjAZ6"
      },
      "source": [
        "### Speeches\n",
        "\n",
        "https://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-election-victory-speech-2010.txt  \n",
        "https://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2006a.txt  \n",
        "https://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2006b.txt  \n",
        "https://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2007.txt  \n",
        "https://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2008.txt  \n",
        "https://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2013.txt  \n",
        "https://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2012.txt  \n",
        "https://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2011.txt  \n",
        "https://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2010.txt  \n",
        "https://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2009.txt  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-YtZHxSjAZ6"
      },
      "outputs": [],
      "source": [
        "speech"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "9C_kwLPljAY0",
        "J4P2wbX2jAZL",
        "CN8gxS-PjAZ2"
      ],
      "name": "W7. Natural Language Processing - Student Version.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit ('3.9.5')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "d34fbd810dd9652f8e464616181cf14dbb258b5c046bed5c2f54c6b5e518fed2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}